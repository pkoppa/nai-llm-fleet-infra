k8s_cluster:

  ## section required for all profiles and environments
  ## kubernetes distribution - supported "nke" "kind"
  distribution: nke
  ## kubernetes cluster name
  name: _required
  ## cluster_profile_type - anything under clusters/_profiles (e.g., llm-management, llm-workloads, etc.)
  profile: _required
  ## environment name - based on profile selected under clusters/_profiles/<profile>/<environment> (e.g., prod, non-prod, etc.)
  environment: _required
  
  ## section required for all profiles and environments
  ## docker hub registry configs
  registry:
    docker_hub:
      user: _required
      password: _required

  ## section required for llm-workloads profiles and environments
  ## nvidia gpu specific configs
  gpu_operator:
    enabled: false
    version: v23.9.0
    cuda_toolkit_version: v1.14.3-centos7
    ## section required for llm-workloads profiles and non-production environments
    ## time slicing typically only configured on dev scenarios. 
    ## ideal for jupyter notebooks
    time_slicing:
      enabled: false
      replica_count: 2

## section required for all profiles and environments
flux:
  ## flux specific configs for github repo
  github:
    repo_url: _required
    repo_user: _required
    repo_api_token: _required

## section required for all profiles and environments
infra:
  ## Global nutanix configs
  nutanix:
    ## Nutanix Prism Creds, required to download NKE creds
    prism_central:
      enabled: true
      endpoint: _required
      user:  _required
      password: _required

    ## section required for llm-management, llm-workloads profiles and environments
    ## Nutanix Objects Store Configs
    objects:
      enabled: false
      # host: _required_if_enabled
      # port: _required_if_enabled
      # region: _required_if_enabled
      # use_ssl: _required_if_enabled
      # access_key: _required_if_enabled
      # secret_key: _required_if_enabled

## section required for all profiles and environments
services:
  ## section required for both all profiles and environments
  #####################################################
  ## Required variables for kube-vip and depedent services
  ## kube-vip specific configs required for any services needing to be configured with LoadBalancer Virtual IP Addresses
  kube_vip:
    enabled: false
    ## Used to configure default global IPAM pool. A minimum of 2 ips should be provide in a range
    ## For Example: ipam_range: 172.20.0.22-172.20.0.23
    ipam_range: _required

  ## required for all platform services that are leveraging nginx-ingress
  nginx_ingress:
    enabled: false
    version: 4.8.3
    ## Virtual IP Address (VIP) dedicated for nginx-ingress controller. 
    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer
    ## Example: vip: 172.20.0.20
    vip: _required
    
    ## NGINX Wildcard Ingress Subdomain used for all default ingress objects created within cluster 
    ## For DEMO purposes, it is common to prefix subdomain with cluster-name as each cluster would require dedicated wildcard domain.
    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.example.com, then value is example.com
    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates. For Example: wildcard_ingress_subdomain: flux-kind-local.172.20.0.20.nip.io
    wildcard_ingress_subdomain: _required

    ## Wildcard Ingress Subdomain for management cluster.
    ## For DEMO purposes, you can leverage the NIP.IO with the nginx_ingress vip and self-signed certificates
    management_cluster_ingress_subdomain: _required

  ## section required only for llm-workloads profiles and environments
  istio:
    enabled: false
    version: 1.17.2
    ## Virtual IP Address (VIP) dedicated for istio ingress gateway. 
    ## This will be used to configure kube-vip IPAM pool to provide Services of Type: LoadBalancer
    ## This address should be mapped to wildcard_ingress_subdomain defined below. For Example: vip: 172.20.0.21
    #vip: _required_if_enabled

    ## Istio Ingress Gateway - Wildcard Subdomain used for all knative/kserve llm inference endpoints. 
    ## EXISTING A Host DNS Records are pre-requisites. Example: If DNS is equal to *.llm-example.com, then value is llm-example.com
    ## For DEMO purposes, you can leverage the NIP.IO with istio.vip and self-signed certificates. For Example: llm-flux-kind-local.172.20.0.21.nip.io
    #wildcard_ingress_subdomain: _required_if_enabled

  ## section required for all profiles and environments
  cert_manager:
    ## By default, cluster issuer will be self-signed-issuer
    ## Disable if existing cluster already has cert_manager and is managed
    enabled: true
    version: v1.13.5
    
    ## section required for all profiles and prod environments
    ## if aws_route53_acme_dns.enabled - the cluster issuer across all services will be set to "letsencrypt-issuer"
    ## Following AWS Route53 Access Creds required for Lets Encrypt ACME DNS Challenge
    ## For additional details, https://cert-manager.io/docs/configuration/acme/dns01/route53/
    ## minimum supported cert-manager version is v1.9.1 https://cert-manager.io/docs/releases/release-notes/release-notes-1.9/#v191
    aws_route53_acme_dns:
      enabled: false
      # email: _required_if_enabled
      # zone: _required_if_enabled
      # hosted_zone_id: _required_if_enabled
      # region: _required_if_enabled
      # key_id: _required_if_enabled
      # key_secret: _required_if_enabled
  

  ## section required for all profiles and environments
  ## do not disable kyverno unless you know what you're doing
  ## this is needed to keep docker hub creds synchronized between namespaces.
  kyverno:
    enabled: true
    version: 3.1.4

  ## the following versions and dependencies kserve are aligned with GPT In A Box Opendocs
  ## the only exception is with cert-manager due to usage of aws route 53
  ## https://opendocs.nutanix.com/gpt-in-a-box/kubernetes/v0.2/getting_started/

  ## section required only for llm-workloads profiles and environments
  ## KServe is used to manage knative-serving/eventing endpoints
  kserve:
    enabled: false
    version: v0.11.2

  ## section required only for llm-workloads profiles and environments
  ## Knative-Serving is used to manage inferencing endpoints
  knative_serving:
    enabled: false
    version: knative-v1.10.1

  ## section required only for llm-workloads profiles and environments
  ## Knative-Istio is used to manage ingress/egress gateways to/from inferencing endpoints
  knative_istio:
    enabled: false
    version: knative-v1.10.0

  ## The following components are leveraged to support Nutanix Validated Designs
  ## The NVD for GPT in a Box leverages a RAG Pipeline with Serverless Functions 
  ## to demonstrate end to end workflow with Nutanix Integration
  
  ## section required for llm-management and llm-workload profiles and environments
  ## Milvus is vector database deployed and configured
  ## It is dependent on Milvus Database and should be unique per management cluster
  ## The GPT NVD Reference application leverages this information on llm-workload
  milvus:
    enabled: false
    version: 4.1.13
    milvus_bucket_name: milvus

  ## section required only for llm-workloads profiles and environments
  ## Knative Eventing is used to manage event driven notifications and connecting to/from various message queues - such as Kafka.
  knative_eventing:
    enabled: false
    version: knative-v1.10.1

  ## section required for all profiles and environments
  ## Kafka is messaging broker used by both knative eventing Document Ingestion serverless function
  ## and integrates with Nutanix Objects Events Notification Kafka Endpoints
  ## Kafka is also leveraged by Milvus as a Messaging Broker for Milvus related events, as opposed to the default Apache Pulsar
  kafka:
    enabled: false
    version: 26.8.5

  ## section required for all profiles and environments
  ## OpenTelemetry Collector version is used for both the Deployment and Daemon is used to collect data for monitoring
  opentelemetry_collector:
    enabled: false
    version: 0.80.1

  ## section required for all profiles and environments
  ## OpenTelemetry Operator is used to deploy opentelemetry components
  opentelemetry_operator:
    enabled: false
    version: 0.47.0

  ## section required only for llm-management profiles and all environments
  ## Uptrace is Observability / Monitoring UI
  uptrace:
    enabled: false
    version: 1.5.7

  ## section required only for llm-workloads profiles and non-prod environments
  ## Jupyterhub is deployed on non-prod workload clusters in NVD Reference
  jupyterhub:
    enabled: false
    version: 3.1.0
    ## default password for admin and user1 accounts 
    default_password: _required_if_enabled
    ## this will merge or override only the inline values defined within platform/jupyterhub/_operators/jupyterhub.yaml, 
    ## but could be overriden by add-ons/patches, so use at your own risk
    ## example below merges hub config with additional users:
    # overrides: |-
    #   hub:
    #     config:
    #       Authenticator:
    #         allowed_users:
    #         - wolfgang
    #         - jesse

  ## section required for all profiles and environments
  weave_gitops:
    enabled: true
    version: 4.0.36
    ## default admin account password
    default_password: _required_if_enabled

## section required only for llm-workloads profiles and environments.
apps:

  ## Required GPT NVD Reference Application Helm Chart Configs
  ## This component is responsible for FrontEnd UI, Vector Embedding, Event Driven Document Ingestion
  gptnvd_reference_app:
    enabled: false
    version: 0.2.7
    ## Nutanix Object Bucket used for RAG Document Ingestion and Event Notifications to Knative-eventing/Kafka Broker
    ## Nutanix Objects Bucket should have kafka endpoint set to `kafka.management_cluster_ingress_subdomain:9096` and Triggering based on at min. `Put and MultiPartUploadComplete` Events
    documents_bucket_name: documents01

  ## Required NAI LLM Helm Chart Configs
  ## This component is responsible for LLM Inferencing Service deployed via Knative-Serving
  nai_helm:
    enabled: false
    version: 0.1.1
    ## default LLM model name and revision that could be leveraged. Default is Llama2_7b_Chat_HF
    ## Another Example:
    ##  model: llama-2-13b-chat
    ##  revision: c2f3ec81aac798ae26dcc57799a994dfbf521496
    model: llama2_7b_chat
    revision: 94b07a6e30c3292b8265ed32ffdeccfdadf434a8
    ## Example Inferencing Parameters, Defaults below, see OpenAI API specification for details/purpose (https://platform.openai.com/)
    maxTokens: 4000
    repPenalty: 1.2
    temperature: 0.2
    topP: 0.9
    useExistingNFS: false
    ## nfs share where llm models are stored. Ex. /llm-model-store
    nfs_export: _required_if_useExistingNFS_IS_TRUE
    ## nfs server FQDN or IP address of Nutanix Files instance
    nfs_server: _required_if_useExistingNFS_IS_TRUE
    ### huggingFaceToken required when useExistingNFS is False. This will download model when llm is initialized
    huggingFaceToken: _required_if_useExistingNFS_IS_FALSE

  